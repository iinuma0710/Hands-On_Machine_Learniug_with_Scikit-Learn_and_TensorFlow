{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章 人工ニューラルネットワーク入門\n",
    "人工ニューラルネットワーク（ANN: Artificial Neural Network）は柔軟・強力かつスケーラブルであり，現代の深層学習の中核をなす技術である．\n",
    "画像分類や音声認識，動画の推薦や強化学習など，様々なタスクに応用されている．\n",
    "\n",
    "## 10.1 生物学的なニューロンから人工ニューロンへ\n",
    "ANN は 1943年に神経生理学者ウォーレン・マカロックと数学者ウォルター・ピッツによって提唱された．彼らは動物の脳の生物学的ニューロンが共同作業で命題論理（propositional logic）を駆使して複雑な計算を実行する仕組みについて，単純な計算モデルを示した．  \n",
    "1960年代頃まで ANN は多くの支持を集めてきたが，その後1980年代はじめまで最初の冬の時代を迎えた．  \n",
    "1980年代には新たなネットワークアーキテクチャの発明と訓練方法の洗練により，再び日の目を見ることになる．  \n",
    "しかし，1990年代までには SVM（サポートベクターマシン）などの発明により，顧みられることはほとんどなくなった．\n",
    "その後，2010年ごろまで長い冬の時代を経て，近年急速に関心を集めるようになった．  \n",
    "その要因としては以下のような要因が考えられる．\n",
    "\n",
    "- ニューラルネットワークを訓練するためのビッグデータ\n",
    "- GPU などの計算能力の大幅な強化\n",
    "- 訓練アルゴリズムの洗練\n",
    "- ANN の理論的限界は実践的にはほとんど無害であることがわかった\n",
    "- ANN が資金を獲得し進歩する良い循環に入った\n",
    "\n",
    "### 10.1.1 生物学的ニューロン\n",
    "ニューロンは大きく細胞体（cell body），樹状突起（dendrite），軸索（axon）の3つからなる．  \n",
    "軸索の先端は終末分枝（terodendria）と呼ばれる多数の枝に別れ，それぞれの分子の末端にあるシナプス終端（synaptic terminal）を介して，ほかのニューロンと接続している．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0122.png\" title=\"生物学的ニューロン\" width=\"50%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "ニューロンは数ミリ秒の内に他のニューロンから十分な数の信号を受けると発火するだけの単純な振る舞いしかしない．\n",
    "しかし，1つのニューロンは数千個の他のニューロンと結びつき，さらにニューロンが数十億個集まることで複雑なネットワークを形成している．\n",
    "下の図はヒトの大脳皮質の例である．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0123.png\" title=\"多層的な生物学的ニューラルネットワーク\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "### 10.1.2 ニューロンによる理論演算\n",
    "ウォーレン・マカロックとウォルター・ピッツは一つ以上のバイナリ入力と一つのバイナリ出力を持ち，一定数以上の入力が活性化すると出力が活性化するという，非常に単純なモデルを提唱した．これが人工ニューロン（artificial neuron）である．  \n",
    "このような単純なモデルでも任意の論理命題を計算するネットワークを構築可能であることが示された．  \n",
    "下に示すのは単純な理論演算を実行する ANN の例である．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0124.png\" title=\"単純な論理演算を実行する ANN\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "### 10.1.3 パーセプトロン\n",
    "パーセプトロン（perceptron）は最も単純な ANN アーキテクチャの一つであり，1957年にフランク・ローゼンブラッドによって考案された．  \n",
    "LTU（linear threshold unit: 線形しきい値素子）と呼ばれる人工ニューロンを基礎としており，入出力地は数値で個々の接続部には重みが与えられている．  \n",
    "LTU の出力は以下の数式で与えられる.\n",
    "$$\n",
    "    h_{\\bf w}({\\bf x})=step({\\bf w} \\cdot {\\bf x})\n",
    "$$\n",
    "ステップ関数 step() にはヘヴィサイドステップ関数や符号関数など，いくつかの種類がある．\n",
    "$$\n",
    "  heaviside(z) = \\begin{cases}\n",
    "    0 & if & z < 0 \\\\\n",
    "    1 & if & z \\geq 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  sgn(z) = \\begin{cases}\n",
    "    -1 & if & z < 0 \\\\\n",
    "    0 & if & z = 0 \\\\\n",
    "    1 & if & z > 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "単純な線形2項分類は，入力の線型結合を計算し，結果がしきい値を超えたら陽クラス，それ以外を陰クラスを出力すればよく，ひとつの LTU で実現できる．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0125.png\" title=\"LTU\" width=\"50%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "パーセプトロンは個々のニューロンがすべての入力に接続される単層の LTU から構成される．  \n",
    "その接続部は入力ニューロン（input neuron）で表現する場合がある．\n",
    "また，一般的にはバイアスフィーチャーが加算される（$x_0 = 1$）．\n",
    "バイアスフィーチャーを出力するニューロンはバイアスニューロン（bias neuron）と呼ぶ．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0126.png\" title=\"パーセプトロン\" width=\"50%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "パーセプトロンの訓練は，「互いに発火する細胞は強く結び付けられる」というヘッブの法則（Hebb's rule）に基づいて行われる．具体的には以下のような入力ニューロンからの重みの更新を行っている．\n",
    "\n",
    "$$\n",
    "    w_{i,j} \\gets w_{i,j} + \\eta (y_j - \\hat{y}_j)x_i\n",
    "$$\n",
    "\n",
    "- $w_{i,j}$はi番目の入力ニューロンとj番目の出力ニューロンを結ぶ接続の重み\n",
    "- $x_i$は現在の訓練インスタンスのi番目の入力値  \n",
    "- $\\hat{y}_j$は現在の訓練インスタンスのj番目の出力ニューロンの出力\n",
    "- $y_j$は現在の訓練インスタンスのj番目の出力ニューロンのターゲット出力\n",
    "- $\\eta$は学習率\n",
    "  \n",
    "パーセプトロンの学習は確率的勾配降下法に非常によく似ており，出力される決定境界は線形である．\n",
    "訓練インスタンが線形分離可能であるとき上記のアルゴリズムの解は収束する．（パーセプトロンの収束定理）\n",
    "また，ロジスティック回帰と異なりパーセプトロンはクラスに属する確率は出力しない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuko/.python-venv/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# iris データセットでパーセプトロンを試す\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # 花弁の長さと幅\n",
    "y = (iris.target == 0).astype(np.int)  # セトナ？\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マービン・ミンスキーとシーモア・パパートは，その著書の中で排他的論理和（XOR）を含む，いくつかのごく単純な問題を解決できないという重大な弱点を指摘している．  \n",
    "そのような解決できない問題の一部は，多層パーセプトロン（MLP: multi-layer perceptron）によって解決できることがわかったが，研究者の落胆は大きく，1970年代以降は推論や問題解決，探索などの研究が盛んに行われるようになった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.4 MLP とバックプロパゲーション\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0127.png\" title=\"XOR 分類問題とこの問題を解決する MLP\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0128.png\" title=\"MLP\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0129.png\" title=\"活性化関数とその導関数\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0130.png\" title=\"分類用の新しい MLP\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
