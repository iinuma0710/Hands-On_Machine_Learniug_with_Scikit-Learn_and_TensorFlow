{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1 インストール\n",
    "```bash\n",
    "$ pip install tensorflow\n",
    "$ pip install tensorflow-gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2 最初のグラフの作成とセッション内での実行\n",
    "TensorFlow では以下のような流れで計算を実行する．\n",
    "- 1. 計算グラフの構築\n",
    "- 2. セッションを開き計算を行う  \n",
    "  \n",
    "計算グラフを構築するだけでは変数の初期化さえ行われていない．セッションを開きオペレーションを CPU や GPU に載せて変数値を保持し計算を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 計算グラフを作る\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x * x * y + y + 2\n",
    "\n",
    "# TensorFlow セッションを開き実際に計算する\n",
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "\n",
    "# セッションを閉じる\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with ブロックでセッションを開いて実行すれば，最後にセッションを閉じる必要はない．  \n",
    "また，sess.run(f) と f.eval() は同じで計算の実行を担っている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# 別の方法で実行する\n",
    "# 最後に close しなくて良い\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.global_variables_initializer() を使えば変数を一括して初期化できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# global_variables_initializer() のインスタンスを作成\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()  # すべての変数を初期化\n",
    "    result = f.eval()\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.InteractiveSession() で自動的にデフォルトセッションを呼び出せるが，最後はマニュアルで close する必要がある．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# IntaractiveSession() を使えば with ブロックは不要\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "\n",
    "# ただし close しないといけない\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.3 グラフの管理\n",
    "作成したノードは自動的にデフォルトグラフに追加される．  \n",
    "複数の計算グラフを管理する場合には，tf.Graph() を呼び出して新しいグラフを作り，with ブロックで一時的にデフォルトグラフに設定する．   \n",
    "tf.reset_default_graph() を呼び出せばデフォルトグラフをリセットできる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "print(x1.graph is tf.get_default_graph())\n",
    "\n",
    "# 複数のグラフを作る場合には、以下のように新しい Graph を作れば良い\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.4 ノードの値のライフサイクル\n",
    "すべてのノードの値はグラフを実行するたびに破棄される．したがって，下記のようなコードでは w と x はそれぞれ2回ずつ計算されることになる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# TensorFlow は自動的にノードの依存関係を判断してくれる\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = y * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # w と x は自動的に評価され、最後に y が計算される\n",
    "    print(y.eval())\n",
    "    # w と x の評価結果は再利用されず、再度計算される\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のような計算方法では w と x を2回評価するため非効率．  \n",
    "下記のように変更することで1回の実行で y と z が同時に評価される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# w と x を2回評価するのは非効率なので、1回の実行で y と z が同時に評価されるようにする\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.5 TensorFlow による線形回帰\n",
    "カリフォルニアの住宅価格データセットを使って線形回帰問題を解いてみる．  \n",
    "線形回帰問題は以下の正規方程式を解くことでパラメータを計算できる．  \n",
    "$$\n",
    "    \\hat{\\theta} = ({\\bf X}^T \\cdot {\\bf X})^{-1} \\cdot {\\bf X}^T \\cdot {\\bf y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カリフォルニアデータセットを使って TensorFlow で線形回帰を試す\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# データの読み込み\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data] # バイアス入力特徴量（x_0 = 1）を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7185181e+01]\n",
      " [ 4.3633747e-01]\n",
      " [ 9.3952334e-03]\n",
      " [-1.0711310e-01]\n",
      " [ 6.4479220e-01]\n",
      " [-4.0338000e-06]\n",
      " [-3.7813708e-03]\n",
      " [-4.2348403e-01]\n",
      " [-4.3721911e-01]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 計算グラフの構築\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "# 線形回帰で求めるパラメータ θ を求める\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    \n",
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn でも同じように線形回帰してみる\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# NumPy でも計算してみる\n",
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.6 勾配降下法の実装\n",
    "バッチ勾配降下法（第４章）を用いながら TensorFlow の勾配降下法について見ていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# データの読み込み\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "\n",
    "# 前処理で scikit-learn を使って標準化を行う\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "# エポック数と学習率の設定\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6.1 マニュアルの勾配降下法\n",
    "- tf.random_uniform() 関数は与えられた形状の乱数を格納したテンソルを生成する\n",
    "- tf.assign() 関数によって $ \\theta_i = \\theta_{i-1} - \\eta\\nabla_{\\theta}MSE(\\theta) $ の式でパラメータを更新する\n",
    "- 100エポックごとに平均二乗誤差の値をチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 3.773333\n",
      "Epoch 100 MSE = 0.8749401\n",
      "Epoch 200 MSE = 0.7483765\n",
      "Epoch 300 MSE = 0.68858063\n",
      "Epoch 400 MSE = 0.6458133\n",
      "Epoch 500 MSE = 0.61450136\n",
      "Epoch 600 MSE = 0.5914934\n",
      "Epoch 700 MSE = 0.5745434\n",
      "Epoch 800 MSE = 0.5620219\n",
      "Epoch 900 MSE = 0.55274415\n",
      "Best theta:\n",
      "[[ 2.0685523 ]\n",
      " [ 0.85882366]\n",
      " [ 0.17053957]\n",
      " [-0.23408358]\n",
      " [ 0.24160214]\n",
      " [ 0.01395802]\n",
      " [-0.04432994]\n",
      " [-0.49494824]\n",
      " [-0.46481228]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 最小二乗法による線形回帰問題の計算グラフの構築\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"pred\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2 / m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "# 計算の実行\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6.2 自動微分を使った方法\n",
    "数式微分（symbolic differentiation）なら偏微分方程式は見つけられるが，面倒で間違いやすい上，得られるコードは必ずしも効率的ではない   \n",
    "　⇛ TensorFlow の自動微分機能を使えば自動的に効率的な方法で勾配計算を行える  \n",
    "<br/>\n",
    "**微分計算の方法**\n",
    "- 数値微分    \n",
    "　実装は簡単だが正確性に欠ける\n",
    "- 数式微分   \n",
    "　正確な計算が可能だが全く別の計算グラフを構築しなければならない\n",
    "- フォワードモード自動微分    \n",
    "　二重数（dual number）を使う\n",
    "- リバースモード自動微分    \n",
    "　出力次元+1回のトレースで正確な計算が可能  \n",
    "\n",
    "<br/>\n",
    "TensorFlow ではリバースモード自動微分が使われている．詳細は付録D（p.509）を参照のこと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 11.8071165\n",
      "Epoch 100 MSE = 0.8375001\n",
      "Epoch 200 MSE = 0.6374426\n",
      "Epoch 300 MSE = 0.6075062\n",
      "Epoch 400 MSE = 0.5876847\n",
      "Epoch 500 MSE = 0.5728895\n",
      "Epoch 600 MSE = 0.56174296\n",
      "Epoch 700 MSE = 0.5533047\n",
      "Epoch 800 MSE = 0.5468861\n",
      "Epoch 900 MSE = 0.5419797\n",
      "Best theta:\n",
      "[[ 2.0685523 ]\n",
      " [ 0.8914852 ]\n",
      " [ 0.15888381]\n",
      " [-0.32904693]\n",
      " [ 0.33475313]\n",
      " [ 0.00929388]\n",
      " [-0.04404037]\n",
      " [-0.5492448 ]\n",
      " [-0.5245189 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 計算グラフ上で gradients = ・・・ を tf.gradients() を使って書き換えるだけ\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"pred\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "# 計算の実行\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "# マニュアル実装したときとほぼ同じ結果が得られる\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6.3 オプティマイザを使うと\n",
    "TensorFlow には様々なオプティマイザが実装済み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 5.8720384\n",
      "Epoch 100 MSE = 0.530826\n",
      "Epoch 200 MSE = 0.52489996\n",
      "Epoch 300 MSE = 0.52439326\n",
      "Epoch 400 MSE = 0.5243305\n",
      "Epoch 500 MSE = 0.5243223\n",
      "Epoch 600 MSE = 0.52432114\n",
      "Epoch 700 MSE = 0.524321\n",
      "Epoch 800 MSE = 0.52432096\n",
      "Epoch 900 MSE = 0.52432096\n",
      "Best theta:\n",
      "[[ 2.0685577 ]\n",
      " [ 0.82962775]\n",
      " [ 0.11875318]\n",
      " [-0.26554263]\n",
      " [ 0.3057092 ]\n",
      " [-0.00450257]\n",
      " [-0.03932657]\n",
      " [-0.89986664]\n",
      " [-0.87052286]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 計算グラフ上で gradients = ・・・ と training_op = ・・・ を書き換え\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"pred\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# この先を書き換える\n",
    "# 単純な勾配降下法\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "# Momentum（こっちのほうが収束が早い）\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# 計算の実行\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "# マニュアル実装したときとほぼ同じ結果が得られる\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.7 訓練アルゴリズムへのデータの供給\n",
    "イテレーションごとにミニバッチを入れ替える必要のあるミニバッチ勾配降下法では，プレースホルダーノードを使って実装すると良い．  \n",
    "プレースホルダーノードでは計算は行われず，計算の実行時に指定したデータを出力する．  \n",
    "一般に TensorFlow に訓練データを渡すときに用いられる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "# placeholder の定義\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))  # None にしておけば可変長入力が取れる\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0714476 ]\n",
      " [ 0.8462012 ]\n",
      " [ 0.11558536]\n",
      " [-0.26835835]\n",
      " [ 0.32982785]\n",
      " [ 0.00608358]\n",
      " [ 0.07052912]\n",
      " [-0.8798858 ]\n",
      " [-0.86342514]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 9.6 で扱った勾配降下法をミニバッチ勾配降下法に書き換える\n",
    "# 計算グラフは最初の2行を書き換え\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "# 以下は変更なし\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"pred\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "# ランダムにバッチサイズの大きさのデータを切り抜いてくる\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)  # 0~m-1 の乱数を batch_size 数分だけ発生\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] \n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch\n",
    "    \n",
    "\n",
    "# 計算の実行\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y:y_batch})\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
