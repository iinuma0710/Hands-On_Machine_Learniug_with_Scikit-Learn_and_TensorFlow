{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章 人工ニューラルネットワーク入門\n",
    "人工ニューラルネットワーク（ANN: Artificial Neural Network）は柔軟・強力かつスケーラブルであり，現代の深層学習の中核をなす技術である．\n",
    "画像分類や音声認識，動画の推薦や強化学習など，様々なタスクに応用されている．\n",
    "\n",
    "## 10.1 生物学的なニューロンから人工ニューロンへ\n",
    "ANN は 1943年に神経生理学者ウォーレン・マカロックと数学者ウォルター・ピッツによって提唱された．彼らは動物の脳の生物学的ニューロンが共同作業で命題論理（propositional logic）を駆使して複雑な計算を実行する仕組みについて，単純な計算モデルを示した．  \n",
    "1960年代頃まで ANN は多くの支持を集めてきたが，その後1980年代はじめまで最初の冬の時代を迎えた．  \n",
    "1980年代には新たなネットワークアーキテクチャの発明と訓練方法の洗練により，再び日の目を見ることになる．  \n",
    "しかし，1990年代までには SVM（サポートベクターマシン）などの発明により，顧みられることはほとんどなくなった．\n",
    "その後，2010年ごろまで長い冬の時代を経て，近年急速に関心を集めるようになった．  \n",
    "その要因としては以下のような要因が考えられる．\n",
    "\n",
    "- ニューラルネットワークを訓練するためのビッグデータ\n",
    "- GPU などの計算能力の大幅な強化\n",
    "- 訓練アルゴリズムの洗練\n",
    "- ANN の理論的限界は実践的にはほとんど無害であることがわかった\n",
    "- ANN が資金を獲得し進歩する良い循環に入った\n",
    "\n",
    "### 10.1.1 生物学的ニューロン\n",
    "ニューロンは大きく細胞体（cell body），樹状突起（dendrite），軸索（axon）の3つからなる．  \n",
    "軸索の先端は終末分枝（terodendria）と呼ばれる多数の枝に別れ，それぞれの分子の末端にあるシナプス終端（synaptic terminal）を介して，ほかのニューロンと接続している．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0122.png\" title=\"生物学的ニューロン\" width=\"50%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "ニューロンは数ミリ秒の内に他のニューロンから十分な数の信号を受けると発火するだけの単純な振る舞いしかしない．\n",
    "しかし，1つのニューロンは数千個の他のニューロンと結びつき，さらにニューロンが数十億個集まることで複雑なネットワークを形成している．\n",
    "下の図はヒトの大脳皮質の例である．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0123.png\" title=\"多層的な生物学的ニューラルネットワーク\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "### 10.1.2 ニューロンによる理論演算\n",
    "ウォーレン・マカロックとウォルター・ピッツは一つ以上のバイナリ入力と一つのバイナリ出力を持ち，一定数以上の入力が活性化すると出力が活性化するという，非常に単純なモデルを提唱した．これが人工ニューロン（artificial neuron）である．  \n",
    "このような単純なモデルでも任意の論理命題を計算するネットワークを構築可能であることが示された．  \n",
    "下に示すのは単純な理論演算を実行する ANN の例である．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0124.png\" title=\"単純な論理演算を実行する ANN\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "### 10.1.3 パーセプトロン\n",
    "パーセプトロン（perceptron）は最も単純な ANN アーキテクチャの一つであり，1957年にフランク・ローゼンブラッドによって考案された．  \n",
    "LTU（linear threshold unit: 線形しきい値素子）と呼ばれる人工ニューロンを基礎としており，入出力地は数値で個々の接続部には重みが与えられている．  \n",
    "LTU の出力は以下の数式で与えられる.\n",
    "$$\n",
    "    h_{\\bf w}({\\bf x})=step({\\bf w} \\cdot {\\bf x})\n",
    "$$\n",
    "ステップ関数 step() にはヘヴィサイドステップ関数や符号関数など，いくつかの種類がある．\n",
    "$$\n",
    "  heaviside(z) = \\begin{cases}\n",
    "    0 & if & z < 0 \\\\\n",
    "    1 & if & z \\geq 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  sgn(z) = \\begin{cases}\n",
    "    -1 & if & z < 0 \\\\\n",
    "    0 & if & z = 0 \\\\\n",
    "    1 & if & z > 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "単純な線形2項分類は，入力の線型結合を計算し，結果がしきい値を超えたら陽クラス，それ以外を陰クラスを出力すればよく，ひとつの LTU で実現できる．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0125.png\" title=\"LTU\" width=\"50%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "パーセプトロンは個々のニューロンがすべての入力に接続される単層の LTU から構成される．  \n",
    "その接続部は入力ニューロン（input neuron）で表現する場合がある．\n",
    "また，一般的にはバイアスフィーチャーが加算される（$x_0 = 1$）．\n",
    "バイアスフィーチャーを出力するニューロンはバイアスニューロン（bias neuron）と呼ぶ．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0126.png\" title=\"パーセプトロン\" width=\"50%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "パーセプトロンの訓練は，「互いに発火する細胞は強く結び付けられる」というヘッブの法則（Hebb's rule）に基づいて行われる．具体的には以下のような入力ニューロンからの重みの更新を行っている．\n",
    "\n",
    "$$\n",
    "    w_{i,j} \\gets w_{i,j} + \\eta (y_j - \\hat{y}_j)x_i\n",
    "$$\n",
    "\n",
    "- $w_{i,j}$はi番目の入力ニューロンとj番目の出力ニューロンを結ぶ接続の重み\n",
    "- $x_i$は現在の訓練インスタンスのi番目の入力値  \n",
    "- $\\hat{y}_j$は現在の訓練インスタンスのj番目の出力ニューロンの出力\n",
    "- $y_j$は現在の訓練インスタンスのj番目の出力ニューロンのターゲット出力\n",
    "- $\\eta$は学習率\n",
    "  \n",
    "パーセプトロンの学習は確率的勾配降下法に非常によく似ており，出力される決定境界は線形である．\n",
    "訓練インスタンが線形分離可能であるとき上記のアルゴリズムの解は収束する．（パーセプトロンの収束定理）\n",
    "また，ロジスティック回帰と異なりパーセプトロンはクラスに属する確率は出力しない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# iris データセットでパーセプトロンを試す\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # 花弁の長さと幅\n",
    "y = (iris.target == 0).astype(np.int)  # セトナ？\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マービン・ミンスキーとシーモア・パパートは，その著書の中で排他的論理和（XOR）を含む，いくつかのごく単純な問題を解決できないという重大な弱点を指摘している．  \n",
    "下図に示すように XOR など解決できない問題の一部は，多層パーセプトロン（MLP: multi-layer perceptron）によって解決できることがわかったが，研究者の落胆は大きく，1970年代以降は推論や問題解決，探索などの研究が盛んに行われるようになった．\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0127.png\" title=\"XOR 分類問題とこの問題を解決する MLP\" width=\"50%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.4 MLP とバックプロパゲーション\n",
    "MLPは一つの入力層と一つ以上の隠れ層，最後に一つの出力層を持つ．  \n",
    "出力層を除く各層にはバイアスニューロンが含まれており，次の層と完全に接続されている．  \n",
    "ANN が複数の隠れ層を持つとき，その ANN を特に深層ニューラルネットワーク（DNN，deep neural network）と呼ぶ．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0128.png\" title=\"MLP\" width=\"50%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "MLP の学習は難しく研究者を長年悩ませてきたが，1986年にD・E・ラメルハートらによって発表された誤差逆伝播法（バックプロパゲーション，backpropagation）の登場によって効率的な訓練が可能となった．  \n",
    "誤差逆伝播法では，\n",
    "- 個々の訓練インスタンスをネットワークに与え，連続する層のすべてのニューロンの出力を計算する．\n",
    "- 次にネットワークの出力誤差を測定する\n",
    "- 最後の隠れ層に含まれる各ニューロンのが各出力誤差にどの程度影響を与えているかを計算する．\n",
    "- 更に一つ前の隠れ層についても同様の計算を行う\n",
    "- これを入力層まで続ける  \n",
    "という流れでネットワークの学習を行う．\n",
    "つまり，ネットワークの逆方向に誤差勾配を伝播していくのである．　　\n",
    "\n",
    "このアルゴリズムを正しく動作させるには，これまで使ってきたステップ関数をある点で微分可能で非ゼロの導関数を持つ別の関数に置き換える必要があった．ステップ関数を置き換える関数（活性化関数）の例としては以下のようなものが挙げられる．  \n",
    "- ロジスティック関数  \n",
    "  値域は0から1  \n",
    "$$\n",
    "    \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "- 双曲線正接（hyperbolic tangent）関数  \n",
    "  値域が−1から＋1なので出力が正規化され収束が早くなる  \n",
    "$$\n",
    "    tanh(z) = 2\\sigma(2z)-1\n",
    "$$\n",
    "- ReLU 関数  \n",
    "  z=0で微分不可能だが，出力の最大値がないため，勾配降下法の問題緩和に役立つ（11章）  \n",
    "$$\n",
    "    ReLU(z) = max(0, z)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0129.png\" title=\"活性化関数とその導関数\" width=\"50%\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "MLP は出力がバイナリクラスのいずれかになるため分類によく使われる．\n",
    "クラスが相互排他的な場合，出力層は個別の活性関数ではなく，下図のようにソフトマックス関数を使う．\n",
    "下図のネットワークは信号の流れが一方向的であるため，順伝播型ニューラルネットワークと呼ばれる．\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/ch10/IMG_0130.png\" title=\"分類用の新しい MLP\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 TensorFlow の高水準 API を使った MLP の訓練\n",
    "scikit-learn 互換の API を提供する TF Learn を用いることで，簡単に MLP を訓練できる．  \n",
    "DNNClassifier クラスを使い，DNN の訓練を行うコードを以下に示す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 12:20:40.225223 140304373921600 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "W0714 12:20:40.767217 140304373921600 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-34e82122bf27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_real_valued_columns_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300, 100], n_classes=10, feature_columns=feature_cols)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdnn_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSKCompat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_clf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "# dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300, 100], n_classes=10, feature_columns=feature_cols)\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf)\n",
    "dnn_clf.fit (X_train, y_train, batch_size=50, steps=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
